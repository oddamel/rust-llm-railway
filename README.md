# Rust LLM Service

A high-performance Rust-based Language Model service designed for Railway deployment.

**Status**: Ready for deployment âœ… (Updated: Aug 15, 2025)

## Features

- **Fast HTTP API** using Actix-web
- **Health Check Endpoint** for monitoring
- **Text Generation API** compatible with OpenAI format
- **CORS Enabled** for web applications
- **Railway Ready** with automatic deployment

## API Endpoints

### Health Check
```
GET /api/health
```

### Text Generation
```
POST /api/v1/inference/text-generation
{
  "prompt": "Your prompt here",
  "max_tokens": 100,
  "temperature": 0.7
}
```

### List Models
```
GET /api/v1/models/list
```

## Local Development

```bash
cargo run
# Service runs on http://127.0.0.1:3200
```

## Railway Deployment

1. Connect your GitHub repository to Railway
2. Railway will automatically detect and build the Rust service
3. The service will be available at your Railway URL

## Environment Variables

- `HOST`: Host to bind to (default: 0.0.0.0)
- `PORT`: Port to run on (Railway sets this automatically)
- `RUST_LOG`: Log level (default: info)

## Used By

- Math School (port 3067)
- Other Nuxt applications in the OAM portfolio